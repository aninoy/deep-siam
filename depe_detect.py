# -*- coding: utf-8 -*-
"""Dupe Detection by Siamese Network(Final).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nspLjqyfLWK96FTnUOB-caxIopusiyb-

## Data load and cleanup
"""

def drive_mount():
  from google.colab import drive
  drive.mount('/content/drive')
  
drive_mount()

from google.colab import files
import json
import os
import pandas as pd
import numpy as np
from itertools import combinations

data_path = '/content/drive/My Drive/dupe_detection'

entity_type = "movie" #@param {type:"string"}

def load_training_dataset(entity_type):
  with open(os.path.join(data_path, 'train.json'), 'r') as f:
    train_df = pd.read_json(f)
    
    train_df = train_df[train_df.entity_type==entity_type]
    train_df.drop(columns=['entity_type', 'episode_number', 'problem_space', 'season_number', 'series_description', 'series_entity_id', 'series_name', 'series_premiere'], inplace=True)
    train_df.drop(columns=['ver_id', 'id', 'source_id'], inplace=True)
  
  return train_df


train_df = load_training_dataset(entity_type)
train_df.head()

"""## genres"""

genre_dict = {'next': 1}


def genre_array_to_dict(row):
  genre_mapping = []
  for g in row:
    next_genre = genre_dict['next']
    g_num = genre_dict.setdefault(g, next_genre)
    if g_num == next_genre:
      genre_dict['next'] += 1
    genre_mapping.append(g_num)
  return genre_mapping
  
genres_df = train_df.genres.str.split(',').apply(lambda x: genre_array_to_dict(x))
genres_df.head()

"""## descriptions"""

from gensim.parsing.preprocessing import preprocess_string, strip_multiple_whitespaces, strip_non_alphanum, strip_punctuation, remove_stopwords
CUSTOM_FILTERS = [lambda x: x.lower(), strip_multiple_whitespaces, strip_non_alphanum, strip_punctuation, remove_stopwords]

def preprocess(row):
  if not row:
    return ''
  return ' '.join(preprocess_string(row, CUSTOM_FILTERS))

description_string_df = train_df.description.apply(lambda x: preprocess(x))
description_string_df.head()

from tensor2tensor.data_generators import text_encoder

description_df = description_string_df.apply(lambda x: text_encoder.ByteTextEncoder().encode(x))
description_df.head()

"""## titles"""

name_df = train_df.name.apply(lambda x: text_encoder.ByteTextEncoder().encode(x))
name_df.head()

"""## dates"""

dates = pd.to_datetime(train_df.premiere)
year_df = dates.dt.year.rename('year')
month_df = dates.dt.month.rename('month')
day_df = dates.dt.day.rename('day')
dates_df = pd.concat([year_df, month_df, day_df], axis=1)
dates_df.head()

"""## preprocessed data"""

data_df = pd.concat([train_df.entity_id, name_df, description_df, genres_df, year_df, month_df, day_df], axis=1).reset_index(drop=True)
data_df.head()

width = max(data_df.description.str.len())

feature_vector = []

for i,row in data_df.iterrows():
  v = np.vstack([np.pad(row['name'], (0, width-len(row['name'])), 'constant', constant_values=0), 
                 np.pad(row.description, (0, width-len(row.description)), 'constant', constant_values=0), 
                 np.pad(row.genres, (0, width-len(row.genres)), 'constant', constant_values=0), 
                 np.pad([row.year, row.month, row.day], (0, width-3), 'constant', constant_values=0)])
  feature_vector.append(v)

# dataset = np.array(feature_vector)
# dataset.shape
np.save(os.path.join(data_path, 'dataset'), dataset)

del feature_vector
del train_df, name_df, genres_df, description_df, description_string_df, year_df

"""## load dataset"""

dataset = np.load(os.path.join(data_path, 'dataset'))

"""## generate positive & negative samples
label : index, index
"""

pos_neg_ratio = 50 #@param {type:"slider", min:0, max:100, step:1}
use_single_groups = False #@param {type:"boolean"}

def add_group_to_samples(g):
  matches = []
  g_array = g.index.data
  for pair in combinations(g_array, 2):
    matches.append([1, pair[0], pair[1]])
  if matches:
    return pd.DataFrame(matches, columns=['label', 'id_1', 'id_2'])
  else:
    return pd.DataFrame([[0, g_array[0], -1]], columns=['label', 'id_1', 'id_2'])
  
raw_samples_df = data_df.groupby(by='entity_id').apply(lambda x: add_group_to_samples(x)).reset_index()

raw_samples_df.head()

pos = raw_samples_df[raw_samples_df.label == 1].shape[0]
no_pair = raw_samples_df[raw_samples_df.label == 0].shape[0]
unique_match_groups = len(raw_samples_df[raw_samples_df.label == 1].groupby(by='entity_id'))
print('total positive samples: ', pos)
print('total representations with no pair: ', no_pair)
print('total groups for negative samples to match against: ', unique_match_groups)

raw_samples = raw_samples_df[['label', 'id_1', 'id_2']].to_numpy()
del raw_samples_df

pos_pairs = raw_samples[raw_samples[:,0] == 1]
negs = np.tile(raw_samples[raw_samples[:,0] == 0][:, 1], 50)
neg_pos = np.random.choice(raw_samples[raw_samples[:,0] == 1,1:].flatten(), size=(negs.shape[0]))
neg_pairs = np.vstack([np.zeros_like(negs), negs, neg_pos]).T

del raw_samples
# data_pairs = np.vstack((raw_samples[raw_samples[:,0] == 1], np.vstack([np.zeros_like(negs), negs, neg_pairs]).T))

del data_df

from sklearn.model_selection import train_test_split

pos_train, pos_not_train = train_test_split(pos_pairs, test_size=.3)
neg_train, neg_not_train = train_test_split(neg_pairs, test_size=.3)


pos_test, pos_dev = train_test_split(pos_not_train, test_size=.5)
neg_test, neg_dev = train_test_split(neg_not_train, test_size=.5)

del pos_not_train, neg_not_train


np.save(os.path.join(data_path, 'train'), np.vstack((pos_train, neg_train)))
np.save(os.path.join(data_path, 'test'), np.vstack((pos_test, neg_test)))
np.save(os.path.join(data_path, 'dev'), np.vstack((pos_dev, neg_dev)))

"""# Siamese Network

### Initialize with imports for keras and tf
"""

import keras
from keras.optimizers import Adam
from keras.layers import Conv2D, Activation, Input, concatenate
from keras.models import Model, Sequential, load_model
from keras.layers.normalization import BatchNormalization
from keras.layers.pooling import MaxPooling2D, AveragePooling2D
from keras.layers.core import Dense, Flatten, Lambda
from keras.regularizers import l2
from keras.engine.topology import Layer
from keras.callbacks import ModelCheckpoint
from keras import backend as K

import tensorflow as tf
import numpy as np
import seaborn as sns
import cv2
import time
import os

# %matplotlib inline
# %load_ext autoreload
# %reload_ext autoreload

# np.set_printoptions(threshold=np.nan)
data_path = '/content/drive/My Drive/dupe_detection'
model_path = os.path.join(data_path, "model.v4.h5")
model_weights_path = os.path.join(data_path, "model_weights_95.h5")

"""### Initializers"""

def initialize_weights(shape, name=None):
    """
        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf
        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01
    """
    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)
  

def initialize_bias(shape, name=None):
    """
        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf
        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01
    """
    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)
  
weights = initialize_weights((1000,1))
sns.distplot(weights)
# plt.title("Plot of weights initialized, with mean of 0.0 and standard deviation of 0.01")
bias = initialize_bias((1000,1))
sns.distplot(bias)
# plt.title("Plot of biases initialized, with mean of 0.0 and standard deviation of 0.01")

class CustomWeightsInitializer:
  def __call__(self, shape):
    return initialize_weights(shape)
  
class CustomBiasInitializer:
  def __call__(self, shape):
    return initialize_bias(shape)

"""### Define the network architecture for siam net"""

def get_encoder_model(input_shape):
  model = Sequential()
  model.add(Conv2D(4, (1,48), activation='relu', input_shape=input_shape, 
                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-3)))
  model.add(MaxPooling2D(pool_size=(1,16), strides=(1,8)))
  model.add(Conv2D(16, (1,24), activation='tanh', input_shape=input_shape, 
                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-3)))
  model.add(MaxPooling2D(pool_size=(1,8), strides=(1,4)))
  model.add(Conv2D(32, (1,12), activation='tanh', input_shape=input_shape, 
                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-3)))
  model.add(MaxPooling2D(pool_size=(1,4), strides=(1,2)))
  model.add(Conv2D(64, (4,1), activation='tanh', kernel_initializer=initialize_weights,
                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-3)))
  model.add(Flatten())
  
  return model

def get_experimental_encoder_model(input_shape):
  model = Sequential()
  model.add(Conv2D(4, (1,48), activation='relu', input_shape=input_shape, 
                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-3)))
  model.add(MaxPooling2D(pool_size=(1,16), strides=(1,8)))
  model.add(Conv2D(16, (1,24), activation='tanh', input_shape=input_shape, 
                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-3)))
  model.add(MaxPooling2D(pool_size=(1,8), strides=(1,4)))
  model.add(Conv2D(32, (4,1), activation='tanh', kernel_initializer=initialize_weights,
                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-3)))
  model.add(Flatten())
  
  return model

def get_siamese_model(input_shape):
    """
        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf
    """
    left_input = Input(input_shape)
    right_input = Input(input_shape)
    
    encoder_model = get_encoder_model(input_shape)
    encoded_l = encoder_model(left_input)
    encoded_r = encoder_model(right_input)
    
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])
    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)
    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)
    
    optimizer = Adam(lr=0.00006)
    siamese_net.compile(loss="binary_crossentropy",optimizer=optimizer, metrics=['accuracy'])
    
    return siamese_net

"""verify the network"""

model = get_siamese_model((4, 3800, 1))
model.summary()
model = None

"""# Training Pipeline"""

def load_dataset():
  dataset = np.load(os.path.join(data_path, 'dataset.npy'))
  return dataset

dataset = load_dataset()

dataset[0:3].T.shape
input_shape = (dataset.shape[1], dataset.shape[2], 1)

class Siamese_Loader:
    """For loading batches and testing tasks to a siamese net"""
    def __init__(self, path, data_subsets = ["train", "dev", "test"]):
        self.data = {}
        self.categories = {}
        self.info = {}
        
        for name in data_subsets:
            file_path = os.path.join(path, name + ".npy")
            print("loading data from {}".format(file_path))
            with open(file_path,"rb") as f:
                data = np.load(f)
                X = data[:, 1:]
                y = data[:, 0]
                self.data[name] = X
                self.categories[name] = y

    def get_batch(self,batch_size,s="train"):
        
        start = 0
        while True:
            end = min(start+batch_size, self.data[s].shape[0])
            p1, p2 = self.data[s][start:end].T
            yield [dataset[p1][...,np.newaxis], dataset[p2][...,np.newaxis]], self.categories[s][start:end]
            if end == self.data[s].shape[0]:
              start = - batch_size
            start += batch_size
    
    def generate(self, batch_size, s="train"):
        """a generator for batches, so model.fit_generator can be used. """
        while True:
            pairs, targets = self.get_batch(batch_size,s)
            yield (pairs, targets)    
    
    def train(self, model, epochs, batch_size=32):
        train_steps = loader.data['train'].shape[0] // batch_size
        validation_steps = loader.data['dev'].shape[0] // batch_size
        callbacks = [ModelCheckpoint(model_path, monitor='val_acc', 
                                                               save_best_only=True)]
        history = model.fit_generator(self.get_batch(batch_size), 
                                      steps_per_epoch=train_steps, epochs=epochs, 
                                      validation_data=self.get_batch(batch_size, s='dev'),
                                      validation_steps=validation_steps,
                                      callbacks=callbacks)
        return history
      
    def evaluate(self, model, batch_size=32):
        steps = loader.data['test'].shape[0] // batch_size
        history = model.evaluate_generator(self.get_batch(batch_size, s='test'), batch_size)
        return history
        
    
loader = Siamese_Loader(data_path)

"""# Evaluation"""

def save_model(model):
  model.save(model_path)

def plot_history(history):
  import matplotlib.pyplot as plt

  # Plot training & validation accuracy values
  plt.plot(history.history['acc'])
  plt.plot(history.history['val_acc'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()

  # Plot training & validation loss values
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()

def evaluate(model, batch_size):
  results = loader.evaluate(model, batch_size=batch_size)
  print("Test results")
  print("--------------")
  print(list(zip(model.metrics_names, results)))

def train_and_evaluate(model, batch_size=32, epochs=10):
  print("Starting training process!")
  print("-------------------------------------")
  
  t_start = time.time()
  
  history = loader.train(model, epochs, batch_size)
  
  print("Time for training: {0}".format(time.time()-t_start))
  
  evaluate(model, batch_size)
  
  return history, model

def load_saved_model(refresh=False):
  
  # check if entire model is saved
  if not refresh and os.path.exists(model_path) and os.path.isfile(model_path):
    custom_objects = {
      'initialize_weights': CustomWeightsInitializer,
      'initialize_bias': CustomBiasInitializer
    }
    print("loading saved model architecture and weights/biases")
    model = load_model(model_path, custom_objects=custom_objects)
  else:
    model = get_siamese_model(input_shape)
    if not refresh and os.path.exists(model_weights_path) and os.path.isfile(model_weights_path):
      print("loading saved model weights")
      model.load_weights(model_weights_path)
    
  return model

model = load_saved_model(refresh=True)
history, model = train_and_evaluate(model, batch_size=64, epochs=500)
plot_history(history)

model = load_saved_model()
evaluate(model, batch_size=64)